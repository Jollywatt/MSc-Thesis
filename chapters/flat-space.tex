\chapter{Calculus in Flat Geometries}

So far, we have been concerned with special relativity at a single point in spacetime.
We move now toward the description of \emph{fields} --- quantities extending across regions of spacetime.
The first step in this direction is the calculus of \emph{flat spacetime}.
In a flat geometry, we may assume that
\begin{itemize}
	\item points in spacetime are elements of a vector space, with differences of points being physically meaningful; and that
	\item fields are parametric functions of a single vector argument representing a location in spacetime.
\end{itemize}
We reserve the word \textdef{field} to mean a map with a fixed vector space codomain.
For instance, the electromagnetic bivector field in flat space $F : \RR^4 → \EA[2]{\RR^4}$ is a function between fixed vector spaces.
In particular, the value of a vector field $F : V → A$ at different points in spacetime can be added; $F(x) + F(y) ∈ A$.

These assumptions are acceptable in special relativity, but in arbitrary regions of spacetime and in the presence of gravity, curvature prevents spacetime from admitting a meaningful vector space structure, and it becomes unphysical to compare field values at different points.
(Consideration of curvature leads to differential geometry and comprises \cref{part:2}.)


\section{Differentiation}


The directional derivative of a vector field $F : V → A$ in the direction $𝒖 ∈ V$ is
\begin{align}
	% ∂_𝒖 F(x) = \lim_{ε → 0}\frac{F(x + ε𝒖) - F(x)}{ε}
	∂_𝒖 F(x) = \dv{ε} \eval{F(x + ε𝒖)}_{ε = 0}
	= \lim_{ε → 0}\frac{F(x + ε𝒖) - F(x)}{ε}
\end{align}
where the point $x ∈ V$ is also a vector.
The directional derivative is linear in $𝒖$, since by a change of variables,
\begin{math}
	∂_{u^a\ve_a}
	= \sum_a \dv{ε} \eval{F(x + εu^a\ve_a)}_{ε = 0}
	= \sum_a u^a \dv{ε'} \eval{F(x + ε'\ve_a)}_{ε' = 0}
	= u^a ∂_{\ve_a}
.\end{math}


Suppose $F : V → A$ is some algebra--valued field.
It is useful to define a kind of ``total'' derivative $\DD F$ which does not depend on a direction of differentiation $𝒖$, but instead encompasses, in a sense, all derivatives in a single object $\DD F : V → A$.
The motivation for this is that it encompasses as special cases the soon-to-be-defined exterior derivative (of exterior algebra) and vector derivative (of geometric algebra).
This derivative will be defined when there is a canonical inclusion $ι : V^* → A$ of dual vectors into the algebra $A$, which is automatic if $A$ is a quotient of $\TA{(V^*)}$.

\begin{definition}
	\label{def:algder}
	Let $F : V → A$ be a field with values in an algebra $A$ with product $⊛$, equipped with an inclusion $ι : V^* → A$.
	The \textdef{algebraic derivative} of $F$ is
	\begin{align}
		\label{eqn:algder}
		\DD F ≔ ι(\ve^a) ⊛ ∂_{\ve_a} F
	\end{align}
	(summation on $a$) where $\set{\ve_a} ⊂ V$ and $\set{\ve^a} ⊂ V^*$ are dual bases.
\end{definition}

To understand this definition, consider the simple case of the free tensor algebra $F : V → \TA{(V^*)}$.
We leave the canonical inclusion $ι : V^* → \TA{(V^*)}$ implicit.
Given a basis $\set{\ve^a} ⊂ V^*$, the algebraic derivative is
\begin{math}
	\DD F = \ve^a ⊗ ∂_a F
,\end{math}
which simply encodes the partial derivatives of a $k$-vector $F$ in a $(k + 1)$-grade object.
In component language,
\begin{math}
	(\DD F)_{a\etc{a_\i}{}k} = ∂_aF_{\etc{a_\i}{}k}
.\end{math}

\subsection{The Exterior Derivative}

Consider a vector field $F : V → \EA{V^*}$ with values in the (dual) exterior algebra.
In this case \cref{eqn:algder} is the \textdef{exterior derivative}
\begin{align}
	\dd F = \dx^a ∧ \pdv{F}{x^a}
\end{align}
where $\set{\dx^a} ⊂ V^*$ also form a dual basis of $\EA{V^*}$.
If $F : V → \EA[k]{V^*}$ is a $k$-vector field, then
\begin{math}
	\dd F = ∂_a F_{\etc{a_\i}{}k} \dx^a ∧ \etc{\dx^{a_\i}}∧k
\end{math}
is a $(k + 1)$-vector.

Using the equivalence of $\EA{V^*}$ with the subspace of antisymmetric tensors (see \cref{sec:exterior-algebra-as-antisymmetric}), the exterior derivative is seen to be the totally anti-symmetrised partial derivative.
In components,
\begin{math}
	(\dd F)_{\etc{a_\i}{}k} = ∂_{[a_1}F_{\etc[2]{a_\i}{}k]}
.\end{math}

The treatment of exterior forms is identical.
An exterior form field $φ : V → \forms[k](V, U)$ is called a $U$-valued \textdef{exterior differential $k$-form}, with exterior derivative defined via its action on vectors
\begin{align}
	(\dd φ)(𝒖, \etc{𝒖_\i},{k})
	&= (\dx^a ∧ ∂_a φ)(𝒖, \etc{𝒖_\i},k)
% \\	&= \sum_{σ ∈ S_{k + 1}} (-1)^σ ∂_{𝒖_{σ(0)}} φ(\etc{𝒖_{σ(\i)}}{}k)
\\	&= \sum_{i = 0}^k (-1)^i ∂_{𝒖_{i}} φ(𝒖_0, ..., \widehat{𝒖_i}, ..., 𝒖_k)
\end{align}
in the Spivak convention (see \cref{sec:exterior-forms}).
Note that the partial derivative acts on the position dependence of $φ$ only --- the vectors $𝒖_i ∈ V$ are fixed input vectors.
This changes when generalising from vector fields of alternating maps to forms defined on a \emph{manifold}, where correction terms are needed to account for partial derivatives of input vectors (discussed in \cref{part:2}).


\subsection{The Vector Derivative}

The algebraic derivative in the tensor and exterior algebras are somewhat uninteresting, because they are easily expressible in component form (e.g., $∂_aF_{\etc{a_\i}{}k}$ or $∂_{[a}F_{\etc{a_\i}{}k]}$).
This is not possible in the geometric algebra, however, because $\GA(V, η)$ is not $\ZZ$-graded, and we would face the problem of notating inhomogeneous objects with a variable number of indices.
The algebraic derivative is, however, still geometrically significant and useful in this case.

In $\GA(V, η)$, the algebraic derivative is called the \textdef{vector derivative}, which we denote $\vd$.
Explicitly, if $F : V → \GA(V, η)$ is a multivector field, then in \cref{eqn:algder} we take $⊛$ to be with the geometric product and take the inclusion to be\sidenote{
	We could just as well consider fields $V → \GA(V^*, η)$, avoiding the need for the isomorphism $\sharp : V^* → V$.
	But the metric is already defined, so we prefer multivectors $\GA(V, η)$ to `dual-multivectors'.
}
\begin{math}
	V^* ∋ 𝒖 ↦ ι(𝒖^\sharp) ∈ \GA(V, η)
.\end{math}
Here, we use the metric to relate $V^* → V$ and the canonical inclusion $ι : V ≡ \GA[1](V, η) → \GA(V, η)$.
The vector derivative is then
\begin{align}
	\vd F = \ve^a \, ∂_{\ve_a} F
\end{align}
(summation on $a$) where $\set{\ve_a} ⊂ V$ and $\set{\ve^a} ⊂ V$ are dual bases, and juxtaposition denotes the geometric product.
If $F$ is a homogeneous $k$-vector, then we may write its components as
\begin{math}
	F = F_{\etc{a_\i}{}k} \etc{\ve^{a_\i}}∧k
\end{math}
and hence
\begin{align}
	\vd F = ∂_{\ve_a}F_{\etc{a_\i}{}k} \, \ve^a(\etc{\ve^{a_\i}}∧k)
.\end{align}
Note that these terms are not $(k + 1)$-blades, but geometric products of vectors $\ve^a$ with $k$-blades --- in general, $(k ± 1)$-multivectors.

We may regard the vector derivative itself as an operator-valued vector,
\begin{align}
	\vd = \ve^a ∂_a
,\end{align}
reflecting the fact that $\vd$ behaves algebraically like a vector.
For instance, the derivative of a vector $𝒖$ has scalar and bivector parts,
\begin{math}
	\vd 𝒖 = \vd \fatdot 𝒖 + \vd ∧ 𝒖
,\end{math}
just like the geometric product of two vectors, $𝒖𝒗 = 𝒖 \fatdot 𝒗 + 𝒖 ∧ 𝒗$.
For a general multivector $F$, then, we have
\begin{align}
	\vd F = \vd \lcontr F + \vd ∧ F
.\end{align}


\subsection{Case Study: Maxwell's Equations}


Expressed in the standard vector calculus of $\RR^3$, Maxwell's equations for the electric $\vb E$ and magnetic $\vb B$ fields in the presence of a source are
\begin{align}
	∇ \cdot \vb E &= \frac{ρ}{ε_0} &&\text{(Gauß' law)}
\\	∇ \cdot \vb B &= 0 &&\text{(Absence of magnetic monopoles)}
\\[1ex]	∇ × \vb E &= -∂_t \vb B &&\text{(Faraday's law)}
\\[1ex]	∇ × \vb B &= μ_0(\vb J + ε_0∂_t \vb E) &&\text{(Ampère's law)}
\end{align}
where $ρ$ is the scalar charge density and $\vb J$ the current density.
The constants $ε_0$ and $μ_0$ are the vacuum permittivity and permeability, respectively, related to the speed of light $c$ by $ε_0μ_0c^2 = 1$.



\begin{margintable}
	\footnotesize
	\begin{tabular}{cl}
		\\
		\multicolumn{2}{c}{\emph{Non-relativistic}} \\
		quantity & dimension \\
		$\vb E$ & $MQ^{-1}LT^{-2}$ \\
		$\vb B$ & $MQ^{-1}T^{-1}$ \\
		$ρ$ & $QL^{-3}$ \\
		$\vb J$ & $QT^{-1}L^{-2}$ \\
		$μ_0$ & $MQ^{-2}L$ \\
		$ε_0$ & $M^{-1}Q^2L^{-3}T^2$ \\
		$∇$, $∂_t$ & $L^{-1}$, $T^{-1}$ \\
		$c$ & $LT^{-1}$ \\
		\\
		\multicolumn{2}{c}{\emph{Relativistic}} \\
		quantity & dimension \\
		$F$ & $MQ^{-1}S^{-1}$ \\
		$J$ & $QS^{-3}$ \\
		$μ_0$, $ε_0^{-1}$ & $MQ^{-2}S$ \\
		$∂$ & $S^{-1}$ \\
		$c$ & $1$ \\
	\end{tabular}
	\caption{
		Dimensions of physical quantities in Maxwell's equations.
		$M$ is mass, $Q$ is electric charge, $T$ is duration and $L$ is length.
		In the relativistic formulation, $T$ and $L$ are unified and replaced by \emph{spacetime interval} $S$.
	}
\end{margintable}

These can be expressed relativistically as eight scalar equations,
\begin{align}
	\label{eqn:maxwells-tensor-form}
	∂_μF^{μν} &= μ_0J^ν
,&	∂_μG^{μν} &= 0
\end{align}
where $F^{μν} = -F^{νμ}$ is the Faraday tensor and $G^{μν}$ its Hodge dual, both encoding the electric and magnetic fields via
\begin{align}
	\label{eqn:components-of-electromagnetic-tensor}
	F^{i0} &= \frac{E^i}{c}
,&	F^{ij} &= -ε^{ijk}B_k
,&	G^{μν} &= \frac12 ε^{μν}{}_{ρσ} F^{ρσ}
% ,&	G^{μν} &= \frac12 η^{μρ}η^{νσ}ε_{ρσλυ} F^{λυ}
,\end{align}
and where $J^μ$ encodes both the static charge density $J^0 = cρ$ and current density $J^i = \vb J$.
The left of eqs.~\eqref{eqn:maxwells-tensor-form} is the \emph{source equation}, while the right is the \emph{second Bianchi identity}.



\begin{proof}
	We show how the relativistic equations \eqref{eqn:maxwells-tensor-form} reduce to the non-relativistic vector calculus equivalents.
	The $0$-component of the source equation is
	$∂_μ F^{μ0} = ∂_iE^i/c = μ_0J^0 = μ_0cρ$ implying $∇ · \vb E = μ_0c^2ρ = ρ/ε_0$ (Gauß' law).
	The $i$-components are
	\begin{align}
		∂_0F^{0i} + ∂_jF^{ji} &= \frac1c∂_t\qty(-\frac{E^i}{c}) - ∂_jε^{jik}B_k = μ_0 J^i
\\		\qqtext{or} ∂_jε^{ijk}B_k &= μ_0 J^i + μ_0ε_0∂_tE^i
	,\end{align}
	which is equivalent to Ampère's law.
	The $0$-component of the Bianchi identity $∂_μG^{μ0} = 0$ is
	\begin{align}
		% \frac12 ε^{i0}{}_{μν}∂_iF^{μν}
		\frac12 ε^i{}_{jk}∂_iF^{jk}
		= -\frac12 ε^i{}_{jk}ε^{jkl}∂_iB_l
		= -∂_iB^i = 0
	,\end{align}
	which using the identity $ε_{ijk}ε^{jkl} = 2δ^l_i$ is $∇ · \vb B = 0$.
	Finally, the $i$-component gives
	\begin{align}
		0 = ∂_μG^{μi} &= \frac12ε^{μi}{}_{ρσ}∂_μF^{ρσ}
		= \frac12ε^{0i}{}_{jk}∂_0F^{jk} + ε^{ji}{}_{k0}∂_jF^{k0}
	\\	&= -\frac14ε^i{}_{jk}ε^{jkl}∂_0B_l - \frac1{2c} ε^{ijk}∂_jE_k
		= -\frac1{2c}\qty(∂_tB^i  + ε^{ijk}∂_jE_k)
	\end{align}
	yielding Faraday's law $∇ × \vb E = -∂_t \vb B$.
\end{proof}



\subsection{With exterior calculus}

It is easy to translate from the language of exterior calculus to the tensor calculus, and hence vice versa, by identifying the former as the subalgebra of totally antisymmetric tensors, as in \cref{sec:exterior-algebra-as-antisymmetric}.
We will employ the Spivak convention, which in particular makes the identification
\begin{align}
	\ve^μ ∧ \ve^ν ≡ \ve^μ ⊗ \ve^ν - \ve^ν ⊗ \ve^μ
\end{align}
where $\ve^μ$ are spacetime basis vectors.







\section{Integration}


\subsection{Stokes' Theorem for Exterior Calculus}

\begin{theorem}[Stokes' theorem in $\RR^n$]
	\label{thm:flat-stokes}
	If $R ⊆ \RR^n$ is a compact $k$-dimensional hypersurface with boundary $∂R$, then a smooth differential form $ω ∈ \forms[k - 1](R)$ satisfies
	\begin{align}
		\label{eqn:stokes}
		\int_R \dd ω = \int_{∂R} ω
	.\end{align}
\end{theorem}
\begin{proof}
	Since $R$ is a $k$-dimensional region with boundary, every point $x ∈ R$ has a neighbourhood diffeomorphic to a neighbourhood of the origin in either $\RR^k$ or $H^k ≔ [0, ∞) ⊕ \RR^{k - 1}$, depending on whether $x$ is an interior point or a boundary point, respectively.

	\begin{marginfigure}
		\centering
		\includefigure[\columnwidth]{stokes-theorem}
		\caption{
			Neighbourhoods in $R$ are diffeomorphic either to interior balls or boundary half-balls.
		}
		\label{fig:stokes-theorem}
	\end{marginfigure}

	Let $\set{U_i}$ be a cover of $R$ consisting of such neighbourhoods.
	Since $R$ is compact, we may assume $\bigcup_{i=1}^N \set{U_i} = R$ to be a finite covering.
	Thus, we have finitely maps $h_i : U_i → X$ where $X$ is either $\RR^k$ or the half-space $H^k$, where $U_i \cong h_i(U_i)$ are diffeomorphic (see \cref{fig:stokes-theorem}).

	Finally, let $\set{ϕ_i : R → [0, 1]}$ be a partition of unity subordinate to $\set{U_i}$, so that $\set{x ∈ R | ϕ_i(x) > 0} ⊆ U_i$ and $ω = \sum_{i=1}^N ϕ_iω$.
	We need only prove the equality \eqref{eqn:stokes} for each $ω_i ≔ ϕ_iω$, and the full result follows be linearity.
	
	The form $h_i^*ω_i ∈ \forms[k - 1](X)$ can be written with respect to canonical coordinates of $X$ as
	\begin{align}
		h_i^*ω_i = \sum_{j=1}^k f_j (-1)^{j - 1} \dd x^{1\cdots\hat{j}\cdots k}
	\end{align}
	using the multi-index notation $\dx^{\etc{i_\i}{}k} ≡ \etc{\dx^{i_\i}}∧k$, where the hat denotes an omitted term.
	The factor of $(-1)^{j - 1}$ gives the $(k - 1)$-form the boundary orientation induced by the volume form $\dx^{\etc\i{}k}$ for convenience.
	Since pullbacks commute with $\dd$,
	\begin{align}
		h^*\ddω_i = \dd(h_i^*ω_i) = \sum_{j=1}^k \pdv{f_j}{x^j} \dd x^{1\cdots n}
	.\end{align}
	There are then two cases to consider.
	\begin{itemize}
		\item \emph{Interior case.}
		If $h_i : U_i → \RR^k$, then the right-hand side of \cref{eqn:stokes} vanishes because $ω_i$ is zero outside the neighbourhood $U_i ⊂ R$ which nowhere meets the boundary $∂R$.
		\begin{align}
			\int_{∂R} ω_i = \int_{∂U_i} ω_i = \int_∅ ω_i = 0
		\end{align}
		The left-hand side evaluates to
		\begin{align}
			\int_R \dd ω_i
			&= \int_X \dd (h_i^*ω_i)
			= \int_{\RR^k} \sum_{j=1}^k \pdv{f_j}{x^j} \dd x^{1\cdots n}
		\\	&= \underbrace{\etc{\int_{-∞}^{+∞}}{}{}}_k \sum_{j=1}^k \pdv{f_j}{x^j} \etc{dx^\i}{}{k}
		\\	&= \underbrace{\etc{\int_{-∞}^{+∞}}{}{}}_{k - 1} \sum_{j=1}^k \eval{f_j}_{x^j=-∞}^{+∞} (-1)^{j-1} dx^1\cdots\widehat{dx^j}\cdots dx^k
			= 0
		,\end{align}
		which vanishes because $h_i^*ω_i$, and hence the $f_j$, vanish outside the neighbourhood $h_i(U_i) ⊂ \RR^k$.

	\item \emph{Boundary case.}
	If $h_i : U_i → H^k$, then the boundary $∂U_i ⊂ ∂R$ is mapped onto the hyperplane $∂H^k = \set{(0, \etc[2]{x^\i},k) | x^j ∈ \RR}$.
	Thus, $dx^1 = 0$ on this boundary, and the right-hand side of \cref{eqn:stokes} becomes
	\begin{align}
		\int_{∂R}ω_i
		&= \int_{∂U_i} h_i^*ω_i
		= -\int_{\RR^{k - 1}} f_1 \etc[2]{dx^\i}{}k
	\\	&= -\underbrace{\etc{\int_{-∞}^{+∞}}{}{}}_{k - 1} f_1(0, \etc[2]{x^\i},k) \etc[2]{dx^\i}{}k
	.\end{align}
	The factor of $-1$ comes from the induced orientation of the boundary $∂H^k$, which is outward-facing, so in the \emph{negative} $x^1$ direction.
	For the left-hand side of \cref{eqn:stokes},
	\begin{align}
		\int_R \dd ω_i
		&= \int_{H^k} h_i^*\ddω_i
		= \int_0^∞ \etc{\int_{-∞}^{+∞}}{}{} \sum_{j=1}^k \pdv{f_j}{x^j} \etc{dx^\i}{}k
	\intertext{
		All terms $\pdv{f_j}{x^j}dx^j$ in the sum for $j > 1$ integrate to boundary terms $x_j → ±∞$ where $f_j$ vanishes.
		This leaves the single term from the integration of $dx^1$,
	}
		&= -\etc{\int_{-∞}^{+∞}}{}{} \eval{f_1}_{x^1=0}^∞ \etc[2]{dx^\i}k
	.\end{align}

	\end{itemize}

	Thus, we have equality for all $ω_i$, so
	\begin{align}
		\int_R \dd ω = \sum_{i=1}^N \int_R \dd ω_i = \sum_{i=1}^N \int_{∂R} ω_i = \int_{∂R} ω
	\end{align}
	by linearity.
\end{proof}

\subsection{Fundamental Theorem of Geometric Calculus}